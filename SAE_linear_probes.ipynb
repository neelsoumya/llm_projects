{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkVnOEhpUOEMlrwewJP64H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelsoumya/llm_projects/blob/main/SAE_linear_probes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAE and linear probe"
      ],
      "metadata": {
        "id": "u0AiPN_bqtWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "vwat9c12q1jN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGjH3JPtqhaz",
        "outputId": "54536e46-20f0-42d7-de30-8932ba3f8f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "TxaDK2-8rRP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sae_and_linear_probe.py\n",
        "\n",
        "PyTorch example:\n",
        " - Train a stacked autoencoder (SAE) on MNIST\n",
        " - Freeze the encoder and train a linear probe on the encoder embeddings\n",
        "\n",
        "Requirements:\n",
        "  pip install torch torchvision tqdm\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# -------------------------\n",
        "# Model: Stacked Autoencoder\n",
        "# -------------------------\n",
        "class StackedAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=28*28, hidden_dims=[1024, 512, 256], bottleneck_dim=64):\n",
        "        \"\"\"\n",
        "        hidden_dims: list, e.g. [1024, 512, 256]\n",
        "        bottleneck_dim: int, dimensionality of the embedding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Encoder layers\n",
        "        enc_layers = []\n",
        "        in_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            enc_layers.append(nn.Linear(in_dim, h))\n",
        "            enc_layers.append(nn.ReLU(inplace=True))\n",
        "            in_dim = h\n",
        "        enc_layers.append(nn.Linear(in_dim, bottleneck_dim))  # final bottleneck (no activation here)\n",
        "        self.encoder = nn.Sequential(*enc_layers)\n",
        "\n",
        "        # Decoder layers (mirror)\n",
        "        dec_layers = []\n",
        "        in_dim = bottleneck_dim\n",
        "        for h in reversed(hidden_dims):\n",
        "            dec_layers.append(nn.Linear(in_dim, h))\n",
        "            dec_layers.append(nn.ReLU(inplace=True))\n",
        "            in_dim = h\n",
        "        dec_layers.append(nn.Linear(in_dim, input_dim))\n",
        "        # We'll output logits and use BCEWithLogitsLoss for stability\n",
        "        self.decoder = nn.Sequential(*dec_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, input_dim) assumed flattened 0..1\n",
        "        z = self.encoder(x)\n",
        "        recon_logits = self.decoder(z)\n",
        "        return recon_logits, z\n",
        "\n",
        "# -------------------------\n",
        "# Utility: training loops\n",
        "# -------------------------\n",
        "def train_autoencoder(model, dataloader, device, epochs=10, lr=1e-3, save_path=None):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # expects logits\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        pbar = tqdm(dataloader, desc=f\"AE Train Epoch {epoch}/{epochs}\")\n",
        "        for imgs, _ in pbar:\n",
        "            imgs = imgs.to(device)\n",
        "            imgs = imgs.view(imgs.size(0), -1)  # flatten\n",
        "            optimizer.zero_grad()\n",
        "            recon_logits, _ = model(imgs)\n",
        "            loss = criterion(recon_logits, imgs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader.dataset)\n",
        "        print(f\"[AE] Epoch {epoch} average loss: {avg_loss:.6f}\")\n",
        "        if save_path:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "def extract_embeddings(model, dataloader, device):\n",
        "    \"\"\"Return (embeddings_tensor, labels_tensor) for dataset in dataloader.\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    all_z = []\n",
        "    all_y = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(dataloader, desc=\"Extract embeddings\"):\n",
        "            imgs = imgs.to(device)\n",
        "            imgs = imgs.view(imgs.size(0), -1)\n",
        "            _, z = model(imgs)\n",
        "            all_z.append(z.cpu())\n",
        "            all_y.append(labels)\n",
        "    embeddings = torch.cat(all_z, dim=0)\n",
        "    labels = torch.cat(all_y, dim=0)\n",
        "    return embeddings, labels\n",
        "\n",
        "def train_linear_probe(encoder, train_loader, val_loader, device, embed_dim, num_classes=10,\n",
        "                       epochs=20, lr=1e-3):\n",
        "    \"\"\"\n",
        "    encoder: encoder module (nn.Module) - expected to output embedding when given flattened input\n",
        "    We assume encoder is already trained (or at least useful).\n",
        "    We'll freeze encoder parameters and train a linear classifier on top.\n",
        "    \"\"\"\n",
        "    # Freeze encoder\n",
        "    encoder.to(device)\n",
        "    for p in encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "    encoder.eval()\n",
        "\n",
        "    # linear probe\n",
        "    probe = nn.Linear(embed_dim, num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        probe.train()\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Probe Train Epoch {epoch}/{epochs}\")\n",
        "        for imgs, labels in pbar:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            imgs = imgs.view(imgs.size(0), -1)\n",
        "            with torch.no_grad():\n",
        "                z = encoder(imgs)  # encoder should return final embedding tensor\n",
        "\n",
        "            logits = probe(z)\n",
        "            loss = criterion(logits, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            running_correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "            pbar.set_postfix(loss=loss.item(), acc=running_correct / total)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = running_correct / total\n",
        "        val_loss, val_acc = evaluate_probe(encoder, probe, val_loader, device, criterion)\n",
        "        print(f\"[Probe] Epoch {epoch} train_loss={train_loss:.4f} train_acc={train_acc:.4f}  \"\n",
        "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return probe\n",
        "\n",
        "def evaluate_probe(encoder, probe, dataloader, device, criterion=None):\n",
        "    encoder.eval()\n",
        "    probe.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            imgs = imgs.view(imgs.size(0), -1)\n",
        "            z = encoder(imgs)\n",
        "            logits = probe(z)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "            if criterion is not None:\n",
        "                loss_sum += criterion(logits, labels).item() * imgs.size(0)\n",
        "    avg_loss = loss_sum / total if (criterion is not None) else 0.0\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "# -------------------------\n",
        "# Small helper: wrapper to use encoder only\n",
        "# -------------------------\n",
        "class EncoderOnly(nn.Module):\n",
        "    def __init__(self, full_ae: StackedAutoencoder):\n",
        "        super().__init__()\n",
        "        self.encoder = full_ae.encoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: flattened\n",
        "        return self.encoder(x)\n",
        "\n",
        "# -------------------------\n",
        "# Main example: MNIST\n",
        "# -------------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # hyperparams\n",
        "    batch_size = 256\n",
        "    ae_epochs = 10\n",
        "    probe_epochs = 15\n",
        "    ae_lr = 1e-3\n",
        "    probe_lr = 1e-3\n",
        "\n",
        "    # Data\n",
        "    transform = transforms.Compose([transforms.ToTensor()])  # values in [0,1]\n",
        "    train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # We'll split train into train/val for the linear probe\n",
        "    val_size = 5000\n",
        "    train_size = len(train_ds) - val_size\n",
        "    train_split, val_split = torch.utils.data.random_split(train_ds, [train_size, val_size])\n",
        "\n",
        "    ae_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    probe_train_loader = DataLoader(train_split, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    probe_val_loader = DataLoader(val_split, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    probe_test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Model\n",
        "    input_dim = 28*28\n",
        "    hidden_dims = [1024, 512, 256]\n",
        "    bottleneck_dim = 64\n",
        "    ae = StackedAutoencoder(input_dim=input_dim, hidden_dims=hidden_dims, bottleneck_dim=bottleneck_dim)\n",
        "\n",
        "    # Train autoencoder\n",
        "    print(\"Training autoencoder...\")\n",
        "    train_autoencoder(ae, ae_loader, device, epochs=ae_epochs, lr=ae_lr, save_path=None)\n",
        "\n",
        "    # Prepare encoder-only wrapper that returns embedding\n",
        "    encoder_only = EncoderOnly(ae)\n",
        "    encoder_only.to(device)\n",
        "\n",
        "    # Train linear probe (freeze encoder)\n",
        "    print(\"Training linear probe (encoder frozen)...\")\n",
        "    probe = train_linear_probe(encoder_only, probe_train_loader, probe_val_loader, device,\n",
        "                               embed_dim=bottleneck_dim, num_classes=10, epochs=probe_epochs, lr=probe_lr)\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_loss, test_acc = evaluate_probe(encoder_only, probe, probe_test_loader, device,\n",
        "                                         criterion=nn.CrossEntropyLoss())\n",
        "    print(f\"Final probe test accuracy: {test_acc:.4f}  test loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Save models\n",
        "    os.makedirs(\"models\", exist_ok=True)\n",
        "    torch.save(ae.state_dict(), \"models/sae.pth\")\n",
        "    torch.save(probe.state_dict(), \"models/linear_probe.pth\")\n",
        "    print(\"Saved models to models/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czokm-VXrSZR",
        "outputId": "56292fed-7e46-4c0f-af1f-10a11be9ddfc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 4.99MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.23MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.64MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training autoencoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAE Train Epoch 1/10:   0%|          | 0/235 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "AE Train Epoch 1/10: 100%|██████████| 235/235 [00:31<00:00,  7.36it/s, loss=0.183]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 1 average loss: 0.239433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 2/10: 100%|██████████| 235/235 [00:34<00:00,  6.74it/s, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 2 average loss: 0.154134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 3/10: 100%|██████████| 235/235 [00:33<00:00,  7.04it/s, loss=0.126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 3 average loss: 0.130474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 4/10: 100%|██████████| 235/235 [00:38<00:00,  6.11it/s, loss=0.118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 4 average loss: 0.120472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 5/10: 100%|██████████| 235/235 [00:38<00:00,  6.14it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 5 average loss: 0.113897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 6/10: 100%|██████████| 235/235 [00:37<00:00,  6.27it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 6 average loss: 0.108816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 7/10: 100%|██████████| 235/235 [00:37<00:00,  6.26it/s, loss=0.102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 7 average loss: 0.104211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 8/10: 100%|██████████| 235/235 [00:37<00:00,  6.31it/s, loss=0.0976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 8 average loss: 0.100317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 9/10: 100%|██████████| 235/235 [00:37<00:00,  6.25it/s, loss=0.0932]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 9 average loss: 0.097454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AE Train Epoch 10/10: 100%|██████████| 235/235 [00:37<00:00,  6.24it/s, loss=0.092]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AE] Epoch 10 average loss: 0.094845\n",
            "Training linear probe (encoder frozen)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 1/15: 100%|██████████| 215/215 [00:10<00:00, 20.16it/s, acc=0.75, loss=0.409]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 1 train_loss=0.9461 train_acc=0.7500  val_loss=0.4379 val_acc=0.9018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 2/15: 100%|██████████| 215/215 [00:11<00:00, 19.36it/s, acc=0.908, loss=0.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 2 train_loss=0.3682 train_acc=0.9081  val_loss=0.3213 val_acc=0.9148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 3/15: 100%|██████████| 215/215 [00:13<00:00, 15.92it/s, acc=0.917, loss=0.305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 3 train_loss=0.3003 train_acc=0.9172  val_loss=0.2825 val_acc=0.9202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 4/15: 100%|██████████| 215/215 [00:10<00:00, 19.94it/s, acc=0.921, loss=0.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 4 train_loss=0.2720 train_acc=0.9213  val_loss=0.2635 val_acc=0.9232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 5/15: 100%|██████████| 215/215 [00:10<00:00, 20.50it/s, acc=0.924, loss=0.241]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 5 train_loss=0.2564 train_acc=0.9239  val_loss=0.2522 val_acc=0.9246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 6/15: 100%|██████████| 215/215 [00:09<00:00, 21.82it/s, acc=0.926, loss=0.246]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 6 train_loss=0.2465 train_acc=0.9258  val_loss=0.2442 val_acc=0.9256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 7/15: 100%|██████████| 215/215 [00:10<00:00, 21.37it/s, acc=0.927, loss=0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 7 train_loss=0.2400 train_acc=0.9273  val_loss=0.2399 val_acc=0.9266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 8/15: 100%|██████████| 215/215 [00:10<00:00, 20.31it/s, acc=0.928, loss=0.223]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 8 train_loss=0.2349 train_acc=0.9283  val_loss=0.2366 val_acc=0.9272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 9/15: 100%|██████████| 215/215 [00:10<00:00, 19.74it/s, acc=0.929, loss=0.271]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 9 train_loss=0.2311 train_acc=0.9291  val_loss=0.2327 val_acc=0.9282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 10/15: 100%|██████████| 215/215 [00:10<00:00, 19.83it/s, acc=0.93, loss=0.221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 10 train_loss=0.2281 train_acc=0.9303  val_loss=0.2299 val_acc=0.9286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 11/15: 100%|██████████| 215/215 [00:10<00:00, 19.87it/s, acc=0.93, loss=0.215]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 11 train_loss=0.2255 train_acc=0.9305  val_loss=0.2283 val_acc=0.9300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 12/15: 100%|██████████| 215/215 [00:10<00:00, 19.97it/s, acc=0.932, loss=0.254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 12 train_loss=0.2234 train_acc=0.9321  val_loss=0.2280 val_acc=0.9288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 13/15: 100%|██████████| 215/215 [00:10<00:00, 19.86it/s, acc=0.932, loss=0.142]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 13 train_loss=0.2217 train_acc=0.9318  val_loss=0.2234 val_acc=0.9312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 14/15: 100%|██████████| 215/215 [00:10<00:00, 21.46it/s, acc=0.933, loss=0.226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 14 train_loss=0.2200 train_acc=0.9328  val_loss=0.2235 val_acc=0.9312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Probe Train Epoch 15/15: 100%|██████████| 215/215 [00:09<00:00, 21.83it/s, acc=0.934, loss=0.323]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] Epoch 15 train_loss=0.2184 train_acc=0.9335  val_loss=0.2229 val_acc=0.9302\n",
            "Final probe test accuracy: 0.9350  test loss: 0.2223\n",
            "Saved models to models/\n"
          ]
        }
      ]
    }
  ]
}